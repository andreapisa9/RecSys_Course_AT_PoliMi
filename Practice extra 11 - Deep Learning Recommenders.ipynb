{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems 2024/25\n",
    "\n",
    "### Practice 9 - Deep Learning Models\n",
    "\n",
    "## The basics of Deep Learning: Multi-Layer Perceptron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movielens10M: Verifying data consistency...\n",
      "Movielens10M: Verifying data consistency... Passed!\n",
      "DataReader: current dataset is: Movielens10M\n",
      "\tNumber of items: 10681\n",
      "\tNumber of users: 69878\n",
      "\tNumber of interactions in URM_all: 10000054\n",
      "\tValue range in URM_all: 0.50-5.00\n",
      "\tInteraction density: 1.34E-02\n",
      "\tInteractions per user:\n",
      "\t\t Min: 2.00E+01\n",
      "\t\t Avg: 1.43E+02\n",
      "\t\t Max: 7.36E+03\n",
      "\tInteractions per item:\n",
      "\t\t Min: 0.00E+00\n",
      "\t\t Avg: 9.36E+02\n",
      "\t\t Max: 3.49E+04\n",
      "\tGini Index: 0.57\n",
      "\n",
      "\tICM name: ICM_tags, Value range: 1.00 / 69.00, Num features: 10106, feature occurrences: 106820, density 9.90E-04\n",
      "\tICM name: ICM_genres, Value range: 1.00 / 1.00, Num features: 20, feature occurrences: 21564, density 1.01E-01\n",
      "\tICM name: ICM_all, Value range: 1.00 / 69.00, Num features: 10126, feature occurrences: 128384, density 1.19E-03\n",
      "\tICM name: ICM_year, Value range: 1.92E+03 / 2.01E+03, Num features: 1, feature occurrences: 10681, density 1.00E+00\n",
      "\n",
      "\n",
      "Warning: 77 (0.11 %) of 69878 users have no sampled items\n",
      "Warning: 266 (0.38 %) of 69878 users have no sampled items\n"
     ]
    }
   ],
   "source": [
    "from Data_manager.split_functions.split_train_validation_random_holdout import split_train_in_two_percentage_global_sample\n",
    "from Data_manager.Movielens.Movielens10MReader import Movielens10MReader\n",
    "\n",
    "data_reader = Movielens10MReader()\n",
    "data_loaded = data_reader.load_data()\n",
    "\n",
    "URM_all = data_loaded.get_URM_all()\n",
    "\n",
    "URM_train_val, URM_test = split_train_in_two_percentage_global_sample(URM_all, 0.8)\n",
    "URM_train, URM_val = split_train_in_two_percentage_global_sample(URM_train_val, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluatorHoldout: Ignoring 77 ( 0.1%) Users that have less than 1 test interactions\n",
      "EvaluatorHoldout: Ignoring 266 ( 0.4%) Users that have less than 1 test interactions\n"
     ]
    }
   ],
   "source": [
    "# Training and testing\n",
    "from Evaluation.Evaluator import EvaluatorHoldout\n",
    "\n",
    "evaluator_test = EvaluatorHoldout(URM_test, [10])\n",
    "evaluator_validation = EvaluatorHoldout(URM_val, [10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "if torch.backends.mps.is_available(): # if torch.cuda.is_available() if you use NVIDIA GPUs\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# create a custom dataset class\n",
    "class IrisDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a custom nn.Module class\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 16)\n",
    "        self.fc2 = nn.Linear(16, 32)\n",
    "        self.fc3 = nn.Linear(32, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 1.118\n",
      "Epoch 2, loss: 1.091\n",
      "Epoch 3, loss: 1.075\n",
      "Epoch 4, loss: 1.060\n",
      "Epoch 5, loss: 1.046\n",
      "Epoch 6, loss: 1.038\n",
      "Epoch 7, loss: 1.020\n",
      "Epoch 8, loss: 1.008\n",
      "Epoch 9, loss: 0.997\n",
      "Epoch 10, loss: 0.988\n",
      "Epoch 11, loss: 0.973\n",
      "Epoch 12, loss: 0.957\n",
      "Epoch 13, loss: 0.937\n",
      "Epoch 14, loss: 0.919\n",
      "Epoch 15, loss: 0.898\n",
      "Epoch 16, loss: 0.872\n",
      "Epoch 17, loss: 0.852\n",
      "Epoch 18, loss: 0.831\n",
      "Epoch 19, loss: 0.816\n",
      "Epoch 20, loss: 0.798\n",
      "Epoch 21, loss: 0.786\n",
      "Epoch 22, loss: 0.766\n",
      "Epoch 23, loss: 0.751\n",
      "Epoch 24, loss: 0.734\n",
      "Epoch 25, loss: 0.717\n",
      "Epoch 26, loss: 0.704\n",
      "Epoch 27, loss: 0.700\n",
      "Epoch 28, loss: 0.678\n",
      "Epoch 29, loss: 0.664\n",
      "Epoch 30, loss: 0.651\n",
      "Epoch 31, loss: 0.642\n",
      "Epoch 32, loss: 0.624\n",
      "Epoch 33, loss: 0.617\n",
      "Epoch 34, loss: 0.605\n",
      "Epoch 35, loss: 0.594\n",
      "Epoch 36, loss: 0.582\n",
      "Epoch 37, loss: 0.578\n",
      "Epoch 38, loss: 0.568\n",
      "Epoch 39, loss: 0.560\n",
      "Epoch 40, loss: 0.549\n",
      "Epoch 41, loss: 0.542\n",
      "Epoch 42, loss: 0.533\n",
      "Epoch 43, loss: 0.526\n",
      "Epoch 44, loss: 0.514\n",
      "Epoch 45, loss: 0.512\n",
      "Epoch 46, loss: 0.502\n",
      "Epoch 47, loss: 0.498\n",
      "Epoch 48, loss: 0.498\n",
      "Epoch 49, loss: 0.487\n",
      "Epoch 50, loss: 0.480\n",
      "Epoch 51, loss: 0.475\n",
      "Epoch 52, loss: 0.469\n",
      "Epoch 53, loss: 0.461\n",
      "Epoch 54, loss: 0.463\n",
      "Epoch 55, loss: 0.451\n",
      "Epoch 56, loss: 0.450\n",
      "Epoch 57, loss: 0.446\n",
      "Epoch 58, loss: 0.438\n",
      "Epoch 59, loss: 0.434\n",
      "Epoch 60, loss: 0.438\n",
      "Epoch 61, loss: 0.430\n",
      "Epoch 62, loss: 0.426\n",
      "Epoch 63, loss: 0.415\n",
      "Epoch 64, loss: 0.418\n",
      "Epoch 65, loss: 0.419\n",
      "Epoch 66, loss: 0.405\n",
      "Epoch 67, loss: 0.402\n",
      "Epoch 68, loss: 0.402\n",
      "Epoch 69, loss: 0.390\n",
      "Epoch 70, loss: 0.386\n",
      "Epoch 71, loss: 0.395\n",
      "Epoch 72, loss: 0.388\n",
      "Epoch 73, loss: 0.384\n",
      "Epoch 74, loss: 0.371\n",
      "Epoch 75, loss: 0.373\n",
      "Epoch 76, loss: 0.371\n",
      "Epoch 77, loss: 0.368\n",
      "Epoch 78, loss: 0.360\n",
      "Epoch 79, loss: 0.358\n",
      "Epoch 80, loss: 0.351\n",
      "Epoch 81, loss: 0.349\n",
      "Epoch 82, loss: 0.355\n",
      "Epoch 83, loss: 0.346\n",
      "Epoch 84, loss: 0.337\n",
      "Epoch 85, loss: 0.340\n",
      "Epoch 86, loss: 0.336\n",
      "Epoch 87, loss: 0.337\n",
      "Epoch 88, loss: 0.330\n",
      "Epoch 89, loss: 0.323\n",
      "Epoch 90, loss: 0.324\n",
      "Epoch 91, loss: 0.324\n",
      "Epoch 92, loss: 0.320\n",
      "Epoch 93, loss: 0.316\n",
      "Epoch 94, loss: 0.312\n",
      "Epoch 95, loss: 0.315\n",
      "Epoch 96, loss: 0.309\n",
      "Epoch 97, loss: 0.308\n",
      "Epoch 98, loss: 0.299\n",
      "Epoch 99, loss: 0.293\n",
      "Epoch 100, loss: 0.296\n",
      "Test loss: 0.289, Accuracy: 98.33\n"
     ]
    }
   ],
   "source": [
    "# create a data loader and model\n",
    "dataset = IrisDataset(X_train, y_train)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "model = MLP()\n",
    "\n",
    "# define a loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# train the model\n",
    "for epoch in range(100):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    print('Epoch %d, loss: %.3f' % (epoch+1, running_loss/(i+1)))\n",
    "\n",
    "# evaluate the model\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data in data_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        test_loss += criterion(outputs, labels).item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / len(dataset)\n",
    "print('Test loss: {:.3f}, Accuracy: {:.2f}'.format(test_loss/(len(data_loader)), accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Recommenders.BaseRecommender import BaseRecommender\n",
    "\n",
    "class DeepLearningRecommender(nn.Module, BaseRecommender):\n",
    "\n",
    "    def __init__(self, URM_train, verbose=True):\n",
    "        super().__init__()\n",
    "        BaseRecommender.__init__(self, URM_train, verbose)\n",
    "        self.device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    def _data_generator(self, batch_size, num_negatives=3, num_items=None):\n",
    "        user_input, item_input, labels = [], [], []\n",
    "        dok_train = URM_train.todok() # <- Dictionary representation of a sparse matrix: allows us to check existing interactions as key-value pairs\n",
    "        if num_items is None : num_items = self.URM_train.shape[1]\n",
    "\n",
    "        self.batch_counter = 0\n",
    "        start = self.batch_counter\n",
    "        stop = min(self.batch_counter + batch_size, len(dok_train.keys()))\n",
    "        for (u,i) in dok_train[start:stop].keys(): # TODO: Too many interactions batched together. Fix start:stop\n",
    "            # positive interaction\n",
    "            user_input.append(u)\n",
    "            item_input.append(i)\n",
    "            labels.append(1) # <- (Implicit ratings)\n",
    "            # negative interactions\n",
    "            for t in range(num_negatives): # <- num_negatives is a hyperparameter\n",
    "                # randomly select an interaction; check if negative\n",
    "                j = np.random.randint(num_items)\n",
    "                while (u,j) in dok_train:\n",
    "                    j = np.random.randint(num_items)\n",
    "                user_input.append(u)\n",
    "                item_input.append(j)\n",
    "                labels.append(0)\n",
    "        self.batch_counter += 1\n",
    "        \n",
    "        user_input = torch.tensor(user_input, dtype=torch.int32, device=self.device)\n",
    "        item_input = torch.tensor(item_input, dtype=torch.int32, device=self.device)\n",
    "        labels = torch.tensor(labels, dtype=torch.int32, device=self.device)\n",
    "        labels = labels.reshape((labels.shape[0],1))\n",
    "        yield user_input, item_input, labels\n",
    "    \n",
    "    def forward(self, user_input, item_input=None):\n",
    "        raise NotImplementedError(\"Forward function not implemented.\")\n",
    "\n",
    "    def fit(self, epochs=30, batch_size=1024, learning_rate=0.0001):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate) # <- The optimizer can be (additionally) considered as a hyperparameter\n",
    "        for i in range(epochs):\n",
    "            for user_input, item_input, labels in self._data_generator(batch_size):\n",
    "                optimizer.zero_grad()\n",
    "                predictions = self.forward(user_input, item_input)\n",
    "                loss = torch.nn.BCELoss().to(self.device) # <- The loss function can be (additionally) considered as a hyperparameter\n",
    "                loss = loss(predictions, labels.float())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            self._print(\"Epoch {} finished. Loss: {}\".format(i, loss.item()))\n",
    "\n",
    "    def _compute_item_score(self, user_id_array, items_to_compute=None):\n",
    "        step = user_id_array.shape[0]\n",
    "        \n",
    "        if items_to_compute is None:\n",
    "            items_to_compute = np.arange(self.URM_train.shape[1], dtype=np.int32)\n",
    "        \n",
    "        predictions = np.empty((step,items_to_compute.shape[0]))\n",
    "        for item in items_to_compute: # <- Parallelization is done by batches of users (could be sub-optimal?)\n",
    "            with torch.no_grad():\n",
    "                predictions[:, item] = self.forward(\n",
    "                    torch.tensor(user_id_array),\n",
    "                    torch.tensor(\n",
    "                        np.ones(step, dtype=np.int32) * item)\n",
    "                    ).cpu().detach().numpy().ravel()\n",
    "        return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoisingAutoencoder(DeepLearningRecommender):\n",
    "\n",
    "    RECOMMENDER_NAME = \"\"\"DENOISING_AUTOENCODER\"\"\"\n",
    "    def __init__(self, URM_train, encoding_dim=69, noise_p=0.1, verbose=True):\n",
    "        super().__init__(URM_train, verbose)\n",
    "        self.noise_p = noise_p\n",
    "        num_items = URM_train.shape[1]\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(num_items, 420),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(420, encoding_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 420),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(420, num_items)\n",
    "        )\n",
    "        self.to(self.device)\n",
    "    \n",
    "    # override: both input and label are batches of user profiles\n",
    "    def _data_generator(self, batch_size):\n",
    "        row_idx = np.arange(self.URM_train.shape[0])\n",
    "        for start in range(0, len(row_idx), batch_size):\n",
    "            end = min(len(row_idx), start + batch_size)\n",
    "            user_input = torch.tensor(self.URM_train[row_idx[start:end],:].toarray(), dtype=torch.float32, device=self.device)\n",
    "            labels = user_input\n",
    "            yield user_input, _, labels\n",
    "\n",
    "    def forward(self, user_input, item_input=None):\n",
    "        # assert(item_input == None, \"Item input not needed\")\n",
    "        noisy_input = self._add_noise(user_input)\n",
    "        encoded = self.encoder(noisy_input)\n",
    "        reconstructed = self.decoder(encoded)\n",
    "        return reconstructed\n",
    "\n",
    "    # override: evaluator passes user profile ids as inputs, we need the\n",
    "    #           full profiles for the forward function to work properly\n",
    "    def _compute_item_score(self, user_id_array, items_to_compute=None):\n",
    "        user_profiles = self.URM_train[user_id_array, :]\n",
    "\n",
    "        if items_to_compute is not None:\n",
    "            mask = np.zeros(self.URM.shape[1], dtype=np.int32)\n",
    "            mask[items_to_compute] = 1\n",
    "            user_profiles = user_profiles[:, mask]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = self.forward(torch.tensor(user_profiles.toarray(), dtype=torch.float32, device=self.device))\n",
    "\n",
    "        return predictions.cpu().detach().numpy()\n",
    "\n",
    "    def _add_noise(self, x):\n",
    "        zeros_mask = np.random.choice([False,True], size=x.shape, p=[1-self.noise_p, self.noise_p])\n",
    "        ones_mask = np.random.choice([False,True], size=x.shape, p=[self.noise_p, 1-self.noise_p])\n",
    "        x[zeros_mask] = 0\n",
    "        x[ones_mask] = 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DENOISING_AUTOENCODER: URM Detected 76 ( 0.7%) items with no interactions.\n",
      "DENOISING_AUTOENCODER: Epoch 0 finished. Loss: 5.801004409790039\n",
      "DENOISING_AUTOENCODER: Epoch 1 finished. Loss: 6.98549222946167\n",
      "DENOISING_AUTOENCODER: Epoch 2 finished. Loss: 1.5783166885375977\n",
      "DENOISING_AUTOENCODER: Epoch 3 finished. Loss: 1.5718098878860474\n",
      "DENOISING_AUTOENCODER: Epoch 4 finished. Loss: 1.5724763870239258\n",
      "DENOISING_AUTOENCODER: Epoch 5 finished. Loss: 2.0860402584075928\n",
      "DENOISING_AUTOENCODER: Epoch 6 finished. Loss: 0.6773252487182617\n",
      "DENOISING_AUTOENCODER: Epoch 7 finished. Loss: 0.5001788139343262\n",
      "DENOISING_AUTOENCODER: Epoch 8 finished. Loss: 0.5002332925796509\n",
      "DENOISING_AUTOENCODER: Epoch 9 finished. Loss: 90.19352722167969\n",
      "EvaluatorHoldout: Processed 69803 (100.0%) in 43.50 sec. Users per second: 1605\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRECISION</th>\n",
       "      <th>PRECISION_RECALL_MIN_DEN</th>\n",
       "      <th>RECALL</th>\n",
       "      <th>MAP</th>\n",
       "      <th>MAP_MIN_DEN</th>\n",
       "      <th>MRR</th>\n",
       "      <th>NDCG</th>\n",
       "      <th>F1</th>\n",
       "      <th>HIT_RATE</th>\n",
       "      <th>ARHR_ALL_HITS</th>\n",
       "      <th>...</th>\n",
       "      <th>COVERAGE_USER</th>\n",
       "      <th>COVERAGE_USER_HIT</th>\n",
       "      <th>USERS_IN_GT</th>\n",
       "      <th>DIVERSITY_GINI</th>\n",
       "      <th>SHANNON_ENTROPY</th>\n",
       "      <th>RATIO_DIVERSITY_HERFINDAHL</th>\n",
       "      <th>RATIO_DIVERSITY_GINI</th>\n",
       "      <th>RATIO_SHANNON_ENTROPY</th>\n",
       "      <th>RATIO_AVERAGE_POPULARITY</th>\n",
       "      <th>RATIO_NOVELTY</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cutoff</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.004884</td>\n",
       "      <td>0.005148</td>\n",
       "      <td>0.001702</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>0.001255</td>\n",
       "      <td>0.011424</td>\n",
       "      <td>0.002684</td>\n",
       "      <td>0.002525</td>\n",
       "      <td>0.047362</td>\n",
       "      <td>0.011707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998927</td>\n",
       "      <td>0.047311</td>\n",
       "      <td>0.998927</td>\n",
       "      <td>0.001035</td>\n",
       "      <td>3.39412</td>\n",
       "      <td>0.903482</td>\n",
       "      <td>0.005317</td>\n",
       "      <td>0.29972</td>\n",
       "      <td>0.157741</td>\n",
       "      <td>0.147761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PRECISION PRECISION_RECALL_MIN_DEN    RECALL       MAP MAP_MIN_DEN  \\\n",
       "cutoff                                                                      \n",
       "10      0.004884                 0.005148  0.001702  0.001202    0.001255   \n",
       "\n",
       "             MRR      NDCG        F1  HIT_RATE ARHR_ALL_HITS  ...  \\\n",
       "cutoff                                                        ...   \n",
       "10      0.011424  0.002684  0.002525  0.047362      0.011707  ...   \n",
       "\n",
       "       COVERAGE_USER COVERAGE_USER_HIT USERS_IN_GT DIVERSITY_GINI  \\\n",
       "cutoff                                                              \n",
       "10          0.998927          0.047311    0.998927       0.001035   \n",
       "\n",
       "       SHANNON_ENTROPY RATIO_DIVERSITY_HERFINDAHL RATIO_DIVERSITY_GINI  \\\n",
       "cutoff                                                                   \n",
       "10             3.39412                   0.903482             0.005317   \n",
       "\n",
       "       RATIO_SHANNON_ENTROPY RATIO_AVERAGE_POPULARITY RATIO_NOVELTY  \n",
       "cutoff                                                               \n",
       "10                   0.29972                 0.157741      0.147761  \n",
       "\n",
       "[1 rows x 27 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denoising_autoencoder = DenoisingAutoencoder(URM_train)\n",
    "\n",
    "denoising_autoencoder.fit(epochs=100, batch_size=1024, learning_rate=0.01)\n",
    "\n",
    "results_df, _ = evaluator_test.evaluateRecommender(denoising_autoencoder)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $EASE^R$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def fit(self, topK=None, l2_norm = 1e3, normalize_matrix = False):\n",
    "\n",
    "        if normalize_matrix:\n",
    "            # Normalize rows and then columns\n",
    "            self.URM_train = normalize(self.URM_train, norm='l2', axis=1)\n",
    "            self.URM_train = normalize(self.URM_train, norm='l2', axis=0)\n",
    "            self.URM_train = sp.csr_matrix(self.URM_train)\n",
    "\n",
    "\n",
    "        # Grahm matrix is X^t X, compute dot product\n",
    "        grahm_matrix = self.URM_train.T.dot(self.URM_train).toarray()\n",
    "\n",
    "        diag_indices = np.diag_indices(grahm_matrix.shape[0])\n",
    "        grahm_matrix[diag_indices] += l2_norm\n",
    "\n",
    "        P = np.linalg.inv(grahm_matrix)\n",
    "\n",
    "        B = P / (-np.diag(P))\n",
    "\n",
    "        B[diag_indices] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EASE_R_Recommender: URM Detected 68 ( 0.6%) items with no interactions.\n",
      "EASE_R_Recommender: Fitting model... \n",
      "EASE_R_Recommender: Fitting model... done in 16.00 sec\n",
      "EvaluatorHoldout: Processed 69801 (100.0%) in 21.88 sec. Users per second: 3190\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRECISION</th>\n",
       "      <th>PRECISION_RECALL_MIN_DEN</th>\n",
       "      <th>RECALL</th>\n",
       "      <th>MAP</th>\n",
       "      <th>MAP_MIN_DEN</th>\n",
       "      <th>MRR</th>\n",
       "      <th>NDCG</th>\n",
       "      <th>F1</th>\n",
       "      <th>HIT_RATE</th>\n",
       "      <th>ARHR_ALL_HITS</th>\n",
       "      <th>...</th>\n",
       "      <th>COVERAGE_USER</th>\n",
       "      <th>COVERAGE_USER_HIT</th>\n",
       "      <th>USERS_IN_GT</th>\n",
       "      <th>DIVERSITY_GINI</th>\n",
       "      <th>SHANNON_ENTROPY</th>\n",
       "      <th>RATIO_DIVERSITY_HERFINDAHL</th>\n",
       "      <th>RATIO_DIVERSITY_GINI</th>\n",
       "      <th>RATIO_SHANNON_ENTROPY</th>\n",
       "      <th>RATIO_AVERAGE_POPULARITY</th>\n",
       "      <th>RATIO_NOVELTY</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cutoff</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.27802</td>\n",
       "      <td>0.32341</td>\n",
       "      <td>0.189994</td>\n",
       "      <td>0.166262</td>\n",
       "      <td>0.189054</td>\n",
       "      <td>0.539328</td>\n",
       "      <td>0.279971</td>\n",
       "      <td>0.225729</td>\n",
       "      <td>0.870288</td>\n",
       "      <td>0.925778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998898</td>\n",
       "      <td>0.869329</td>\n",
       "      <td>0.998898</td>\n",
       "      <td>0.025356</td>\n",
       "      <td>8.333874</td>\n",
       "      <td>0.994708</td>\n",
       "      <td>0.130267</td>\n",
       "      <td>0.735921</td>\n",
       "      <td>1.688469</td>\n",
       "      <td>0.091234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PRECISION PRECISION_RECALL_MIN_DEN    RECALL       MAP MAP_MIN_DEN  \\\n",
       "cutoff                                                                      \n",
       "10       0.27802                  0.32341  0.189994  0.166262    0.189054   \n",
       "\n",
       "             MRR      NDCG        F1  HIT_RATE ARHR_ALL_HITS  ...  \\\n",
       "cutoff                                                        ...   \n",
       "10      0.539328  0.279971  0.225729  0.870288      0.925778  ...   \n",
       "\n",
       "       COVERAGE_USER COVERAGE_USER_HIT USERS_IN_GT DIVERSITY_GINI  \\\n",
       "cutoff                                                              \n",
       "10          0.998898          0.869329    0.998898       0.025356   \n",
       "\n",
       "       SHANNON_ENTROPY RATIO_DIVERSITY_HERFINDAHL RATIO_DIVERSITY_GINI  \\\n",
       "cutoff                                                                   \n",
       "10            8.333874                   0.994708             0.130267   \n",
       "\n",
       "       RATIO_SHANNON_ENTROPY RATIO_AVERAGE_POPULARITY RATIO_NOVELTY  \n",
       "cutoff                                                               \n",
       "10                  0.735921                 1.688469      0.091234  \n",
       "\n",
       "[1 rows x 27 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Recommenders.EASE_R.EASE_R_Recommender import EASE_R_Recommender\n",
    "\n",
    "model = EASE_R_Recommender(URM_train)\n",
    "\n",
    "model.fit() # <- hyperparams left to default value, obviously could (and should) be optimized\n",
    "\n",
    "results_df, _ = evaluator_test.evaluateRecommender(model)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Tower Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variant 1:\n",
    "# 1. Make 2 embeddings of equal dimensions and concatenate\n",
    "# 2. Couple of Dense layers\n",
    "# 3. Obtain prediction (single score)\n",
    "class TwoTowerRecommender_type1(DeepLearningRecommender):\n",
    "    \n",
    "    RECOMMENDER_NAME = \"\"\"TWO_TOWER_1\"\"\"\n",
    "    \n",
    "    def __init__(self, URM_train, num_users, num_items, layers=[10], reg_layers=[0], verbose = True):\n",
    "        super().__init__(URM_train, verbose)\n",
    "        self.mlp_embedding_user = nn.Embedding(num_users, int(layers[0]/2), device=self.device)\n",
    "        self.mlp_embedding_item = nn.Embedding(num_items, int(layers[0]/2), device=self.device)\n",
    "\n",
    "        self.mlp_layers = nn.ModuleList([\n",
    "            nn.Linear(layers[i-1], layers[i], bias=True, device=self.device) for i in range(1, len(layers))\n",
    "            ])\n",
    "        for i, layer in enumerate(self.mlp_layers):\n",
    "            nn.init.normal_(layer.weight)\n",
    "            layer.bias.data.zero_()\n",
    "            layer.weight_decay = reg_layers[i]\n",
    "\n",
    "        self.prediction_layer = nn.Linear(layers[-1], 1, bias=True, device=self.device)\n",
    "        nn.init.uniform_(self.prediction_layer.weight)\n",
    "        self.prediction_layer.bias.data.zero_()\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, user_input, item_input):\n",
    "        mlp_user_latent = self.mlp_embedding_user(user_input.long().to(self.device))\n",
    "        mlp_item_latent = self.mlp_embedding_item(item_input.long().to(self.device))\n",
    "        mlp_vector = torch.cat((mlp_user_latent, mlp_item_latent), dim=1)\n",
    "        for layer in self.mlp_layers:\n",
    "            mlp_vector = torch.relu(layer(mlp_vector))\n",
    "\n",
    "        predict_vector = mlp_vector\n",
    "        prediction = torch.sigmoid(self.prediction_layer(predict_vector))\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variant 2:\n",
    "# 1. Couple of Dense layers process user/item profiles\n",
    "# 2. Merge and final Dense layer to obtain prediciton\n",
    "class TwoTowerRecommender_type2(DeepLearningRecommender):\n",
    "\n",
    "    RECOMMENDER_NAME = \"\"\"TWO_TOWER_2\"\"\"\n",
    "\n",
    "    def __init__(self, URM_train, num_users, num_items, layers=[10], reg_layers=[0], verbose = True):\n",
    "        super().__init__(URM_train, verbose)\n",
    "        layers[0] = int(layers[0]/2) # <- The first layer is split in two tower inputs at the beginning\n",
    "        self.mlp_embedding_user = nn.Embedding(num_users, layers[0], device=self.device)\n",
    "        self.mlp_embedding_item = nn.Embedding(num_items, layers[0], device=self.device) # <- It's possible to make the towers asymmetric! Mind the output dimension though\n",
    "\n",
    "        self.mlp_layers_tower1 = nn.ModuleList([\n",
    "            nn.Linear(\n",
    "                layers[i-1],\n",
    "                layers[i], bias=True, device=self.device\n",
    "                ) for i in range(1, len(layers))\n",
    "            ])\n",
    "        \n",
    "        self.mlp_layers_tower2 = nn.ModuleList([\n",
    "            nn.Linear(\n",
    "                layers[i-1],\n",
    "                layers[i], bias=True, device=self.device\n",
    "                ) for i in range(1, len(layers))\n",
    "            ])\n",
    "        \n",
    "        for i, layer in enumerate(self.mlp_layers_tower1):\n",
    "            nn.init.normal_(layer.weight)\n",
    "            layer.bias.data.zero_()\n",
    "            layer.weight_decay = reg_layers[i]\n",
    "\n",
    "        for i, layer in enumerate(self.mlp_layers_tower2):\n",
    "            nn.init.normal_(layer.weight)\n",
    "            layer.bias.data.zero_()\n",
    "            layer.weight_decay = reg_layers[i]\n",
    "\n",
    "        self.prediction_layer = nn.Linear(layers[-1], 1, bias=True, device=self.device)\n",
    "        nn.init.uniform_(self.prediction_layer.weight)\n",
    "        self.prediction_layer.bias.data.zero_()\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, user_input, item_input):\n",
    "        mlp_user_latent = self.mlp_embedding_user(user_input.long().to(self.device))\n",
    "        mlp_item_latent = self.mlp_embedding_item(item_input.long().to(self.device))\n",
    "\n",
    "        mlp_user_vector = mlp_user_latent\n",
    "        mlp_item_vector = mlp_item_latent\n",
    "\n",
    "        for layer in self.mlp_layers_tower1:\n",
    "            mlp_user_vector = torch.relu(layer(mlp_user_vector))\n",
    "\n",
    "        for layer in self.mlp_layers_tower2:\n",
    "            mlp_item_vector = torch.relu(layer(mlp_item_vector))\n",
    "\n",
    "        predict_vector = mlp_user_vector * mlp_item_vector # <- Merge the tensors via element-wise multiplication\n",
    "        prediction = torch.sigmoid(self.prediction_layer(predict_vector))\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TWO_TOWER_1: URM Detected 76 ( 0.7%) items with no interactions.\n",
      "TWO_TOWER_1: Epoch 0 finished. Loss: 1.423458218574524\n",
      "TWO_TOWER_1: Epoch 1 finished. Loss: 1.334832787513733\n",
      "TWO_TOWER_1: Epoch 2 finished. Loss: 1.2557604312896729\n",
      "TWO_TOWER_1: Epoch 3 finished. Loss: 1.181513786315918\n",
      "TWO_TOWER_1: Epoch 4 finished. Loss: 1.1167007684707642\n",
      "TWO_TOWER_1: Epoch 5 finished. Loss: 1.0595446825027466\n",
      "TWO_TOWER_1: Epoch 6 finished. Loss: 1.0078457593917847\n",
      "TWO_TOWER_1: Epoch 7 finished. Loss: 0.959753155708313\n",
      "TWO_TOWER_1: Epoch 8 finished. Loss: 0.9204481244087219\n",
      "TWO_TOWER_1: Epoch 9 finished. Loss: 0.8844990730285645\n",
      "TWO_TOWER_1: Epoch 10 finished. Loss: 0.8526425361633301\n",
      "TWO_TOWER_1: Epoch 11 finished. Loss: 0.8253729939460754\n",
      "TWO_TOWER_1: Epoch 12 finished. Loss: 0.8013153672218323\n",
      "TWO_TOWER_1: Epoch 13 finished. Loss: 0.7806523442268372\n",
      "TWO_TOWER_1: Epoch 14 finished. Loss: 0.763870894908905\n",
      "TWO_TOWER_1: Epoch 15 finished. Loss: 0.7467600107192993\n",
      "TWO_TOWER_1: Epoch 16 finished. Loss: 0.7319393157958984\n",
      "TWO_TOWER_1: Epoch 17 finished. Loss: 0.7209361791610718\n",
      "TWO_TOWER_1: Epoch 18 finished. Loss: 0.7100390195846558\n",
      "TWO_TOWER_1: Epoch 19 finished. Loss: 0.6996669769287109\n",
      "TWO_TOWER_1: Epoch 20 finished. Loss: 0.691947340965271\n",
      "TWO_TOWER_1: Epoch 21 finished. Loss: 0.6849662661552429\n",
      "TWO_TOWER_1: Epoch 22 finished. Loss: 0.678074061870575\n",
      "TWO_TOWER_1: Epoch 23 finished. Loss: 0.6724894046783447\n",
      "TWO_TOWER_1: Epoch 24 finished. Loss: 0.667331874370575\n",
      "TWO_TOWER_1: Epoch 25 finished. Loss: 0.663081705570221\n",
      "TWO_TOWER_1: Epoch 26 finished. Loss: 0.6583437323570251\n",
      "TWO_TOWER_1: Epoch 27 finished. Loss: 0.654448926448822\n",
      "TWO_TOWER_1: Epoch 28 finished. Loss: 0.650858998298645\n",
      "TWO_TOWER_1: Epoch 29 finished. Loss: 0.6473381519317627\n",
      "TWO_TOWER_1: Epoch 30 finished. Loss: 0.6449646353721619\n",
      "TWO_TOWER_1: Epoch 31 finished. Loss: 0.6418749094009399\n",
      "TWO_TOWER_1: Epoch 32 finished. Loss: 0.6390758752822876\n",
      "TWO_TOWER_1: Epoch 33 finished. Loss: 0.6368576884269714\n",
      "TWO_TOWER_1: Epoch 34 finished. Loss: 0.6344443559646606\n",
      "TWO_TOWER_1: Epoch 35 finished. Loss: 0.6322292685508728\n",
      "TWO_TOWER_1: Epoch 36 finished. Loss: 0.6302987337112427\n",
      "TWO_TOWER_1: Epoch 37 finished. Loss: 0.6284053325653076\n",
      "TWO_TOWER_1: Epoch 38 finished. Loss: 0.626266360282898\n",
      "TWO_TOWER_1: Epoch 39 finished. Loss: 0.6243594288825989\n",
      "TWO_TOWER_1: Epoch 40 finished. Loss: 0.6227790117263794\n",
      "TWO_TOWER_1: Epoch 41 finished. Loss: 0.6210542917251587\n",
      "TWO_TOWER_1: Epoch 42 finished. Loss: 0.6195218563079834\n",
      "TWO_TOWER_1: Epoch 43 finished. Loss: 0.6179649233818054\n",
      "TWO_TOWER_1: Epoch 44 finished. Loss: 0.6165937781333923\n",
      "TWO_TOWER_1: Epoch 45 finished. Loss: 0.6151092052459717\n",
      "TWO_TOWER_1: Epoch 46 finished. Loss: 0.6137468814849854\n",
      "TWO_TOWER_1: Epoch 47 finished. Loss: 0.6121900677680969\n",
      "TWO_TOWER_1: Epoch 48 finished. Loss: 0.6108355522155762\n",
      "TWO_TOWER_1: Epoch 49 finished. Loss: 0.6096036434173584\n",
      "TWO_TOWER_1: Epoch 50 finished. Loss: 0.6082549691200256\n",
      "TWO_TOWER_1: Epoch 51 finished. Loss: 0.606999397277832\n",
      "TWO_TOWER_1: Epoch 52 finished. Loss: 0.6057762503623962\n",
      "TWO_TOWER_1: Epoch 53 finished. Loss: 0.6045740842819214\n",
      "TWO_TOWER_1: Epoch 54 finished. Loss: 0.6033498048782349\n",
      "TWO_TOWER_1: Epoch 55 finished. Loss: 0.6021630167961121\n",
      "TWO_TOWER_1: Epoch 56 finished. Loss: 0.6009268760681152\n",
      "TWO_TOWER_1: Epoch 57 finished. Loss: 0.5997698903083801\n",
      "TWO_TOWER_1: Epoch 58 finished. Loss: 0.5985973477363586\n",
      "TWO_TOWER_1: Epoch 59 finished. Loss: 0.5975251793861389\n",
      "TWO_TOWER_1: Epoch 60 finished. Loss: 0.5964809060096741\n",
      "TWO_TOWER_1: Epoch 61 finished. Loss: 0.5954153537750244\n",
      "TWO_TOWER_1: Epoch 62 finished. Loss: 0.5941967964172363\n",
      "TWO_TOWER_1: Epoch 63 finished. Loss: 0.5931949019432068\n",
      "TWO_TOWER_1: Epoch 64 finished. Loss: 0.5921467542648315\n",
      "TWO_TOWER_1: Epoch 65 finished. Loss: 0.59096360206604\n",
      "TWO_TOWER_1: Epoch 66 finished. Loss: 0.5899538397789001\n",
      "TWO_TOWER_1: Epoch 67 finished. Loss: 0.5888674855232239\n",
      "TWO_TOWER_1: Epoch 68 finished. Loss: 0.5878933668136597\n",
      "TWO_TOWER_1: Epoch 69 finished. Loss: 0.5867407917976379\n",
      "TWO_TOWER_1: Epoch 70 finished. Loss: 0.5858080387115479\n",
      "TWO_TOWER_1: Epoch 71 finished. Loss: 0.5846953988075256\n",
      "TWO_TOWER_1: Epoch 72 finished. Loss: 0.58359295129776\n",
      "TWO_TOWER_1: Epoch 73 finished. Loss: 0.582604706287384\n",
      "TWO_TOWER_1: Epoch 74 finished. Loss: 0.5815522074699402\n",
      "TWO_TOWER_1: Epoch 75 finished. Loss: 0.5804668664932251\n",
      "TWO_TOWER_1: Epoch 76 finished. Loss: 0.5792989134788513\n",
      "TWO_TOWER_1: Epoch 77 finished. Loss: 0.5783039331436157\n",
      "TWO_TOWER_1: Epoch 78 finished. Loss: 0.5771723985671997\n",
      "TWO_TOWER_1: Epoch 79 finished. Loss: 0.5760630965232849\n",
      "TWO_TOWER_1: Epoch 80 finished. Loss: 0.5749780535697937\n",
      "TWO_TOWER_1: Epoch 81 finished. Loss: 0.5739483833312988\n",
      "TWO_TOWER_1: Epoch 82 finished. Loss: 0.5728333592414856\n",
      "TWO_TOWER_1: Epoch 83 finished. Loss: 0.5714806914329529\n",
      "TWO_TOWER_1: Epoch 84 finished. Loss: 0.5704022645950317\n",
      "TWO_TOWER_1: Epoch 85 finished. Loss: 0.5690992474555969\n",
      "TWO_TOWER_1: Epoch 86 finished. Loss: 0.5681689977645874\n",
      "TWO_TOWER_1: Epoch 87 finished. Loss: 0.5669458508491516\n",
      "TWO_TOWER_1: Epoch 88 finished. Loss: 0.5656367540359497\n",
      "TWO_TOWER_1: Epoch 89 finished. Loss: 0.5643730759620667\n",
      "TWO_TOWER_1: Epoch 90 finished. Loss: 0.563014030456543\n",
      "TWO_TOWER_1: Epoch 91 finished. Loss: 0.5616913437843323\n",
      "TWO_TOWER_1: Epoch 92 finished. Loss: 0.5604777932167053\n",
      "TWO_TOWER_1: Epoch 93 finished. Loss: 0.5590242147445679\n",
      "TWO_TOWER_1: Epoch 94 finished. Loss: 0.5576415061950684\n",
      "TWO_TOWER_1: Epoch 95 finished. Loss: 0.5562150478363037\n",
      "TWO_TOWER_1: Epoch 96 finished. Loss: 0.5547623634338379\n",
      "TWO_TOWER_1: Epoch 97 finished. Loss: 0.5533096194267273\n",
      "TWO_TOWER_1: Epoch 98 finished. Loss: 0.5520431399345398\n",
      "TWO_TOWER_1: Epoch 99 finished. Loss: 0.5504394769668579\n",
      "TWO_TOWER_1: Epoch 100 finished. Loss: 0.5487346053123474\n",
      "TWO_TOWER_1: Epoch 101 finished. Loss: 0.5474534630775452\n",
      "TWO_TOWER_1: Epoch 102 finished. Loss: 0.5461305975914001\n",
      "TWO_TOWER_1: Epoch 103 finished. Loss: 0.5442508459091187\n",
      "TWO_TOWER_1: Epoch 104 finished. Loss: 0.5430485010147095\n",
      "TWO_TOWER_1: Epoch 105 finished. Loss: 0.5413165092468262\n",
      "TWO_TOWER_1: Epoch 106 finished. Loss: 0.539486825466156\n",
      "TWO_TOWER_1: Epoch 107 finished. Loss: 0.5379050374031067\n",
      "TWO_TOWER_1: Epoch 108 finished. Loss: 0.5363112092018127\n",
      "TWO_TOWER_1: Epoch 109 finished. Loss: 0.5348522067070007\n",
      "TWO_TOWER_1: Epoch 110 finished. Loss: 0.5330537557601929\n",
      "TWO_TOWER_1: Epoch 111 finished. Loss: 0.5316309928894043\n",
      "TWO_TOWER_1: Epoch 112 finished. Loss: 0.5299851298332214\n",
      "TWO_TOWER_1: Epoch 113 finished. Loss: 0.5286946296691895\n",
      "TWO_TOWER_1: Epoch 114 finished. Loss: 0.5269379615783691\n",
      "TWO_TOWER_1: Epoch 115 finished. Loss: 0.525531530380249\n",
      "TWO_TOWER_1: Epoch 116 finished. Loss: 0.5234912633895874\n",
      "TWO_TOWER_1: Epoch 117 finished. Loss: 0.5223793387413025\n",
      "TWO_TOWER_1: Epoch 118 finished. Loss: 0.5206599831581116\n",
      "TWO_TOWER_1: Epoch 119 finished. Loss: 0.5196852684020996\n",
      "TWO_TOWER_1: Epoch 120 finished. Loss: 0.5180149674415588\n",
      "TWO_TOWER_1: Epoch 121 finished. Loss: 0.5167046189308167\n",
      "TWO_TOWER_1: Epoch 122 finished. Loss: 0.5150030851364136\n",
      "TWO_TOWER_1: Epoch 123 finished. Loss: 0.5138586759567261\n",
      "TWO_TOWER_1: Epoch 124 finished. Loss: 0.5118342638015747\n",
      "TWO_TOWER_1: Epoch 125 finished. Loss: 0.5107260346412659\n",
      "TWO_TOWER_1: Epoch 126 finished. Loss: 0.510167121887207\n",
      "TWO_TOWER_1: Epoch 127 finished. Loss: 0.5079818964004517\n",
      "TWO_TOWER_1: Epoch 128 finished. Loss: 0.5075938105583191\n",
      "TWO_TOWER_1: Epoch 129 finished. Loss: 0.5055623650550842\n",
      "TWO_TOWER_1: Epoch 130 finished. Loss: 0.5044505596160889\n",
      "TWO_TOWER_1: Epoch 131 finished. Loss: 0.5033579468727112\n",
      "TWO_TOWER_1: Epoch 132 finished. Loss: 0.5017743706703186\n",
      "TWO_TOWER_1: Epoch 133 finished. Loss: 0.5011457800865173\n",
      "TWO_TOWER_1: Epoch 134 finished. Loss: 0.5001059770584106\n",
      "TWO_TOWER_1: Epoch 135 finished. Loss: 0.4981347918510437\n",
      "TWO_TOWER_1: Epoch 136 finished. Loss: 0.49750933051109314\n",
      "TWO_TOWER_1: Epoch 137 finished. Loss: 0.49667927622795105\n",
      "TWO_TOWER_1: Epoch 138 finished. Loss: 0.49541327357292175\n",
      "TWO_TOWER_1: Epoch 139 finished. Loss: 0.49419716000556946\n",
      "TWO_TOWER_1: Epoch 140 finished. Loss: 0.49259692430496216\n",
      "TWO_TOWER_1: Epoch 141 finished. Loss: 0.4918772280216217\n",
      "TWO_TOWER_1: Epoch 142 finished. Loss: 0.48976412415504456\n",
      "TWO_TOWER_1: Epoch 143 finished. Loss: 0.4896703362464905\n",
      "TWO_TOWER_1: Epoch 144 finished. Loss: 0.4886627793312073\n",
      "TWO_TOWER_1: Epoch 145 finished. Loss: 0.48767614364624023\n",
      "TWO_TOWER_1: Epoch 146 finished. Loss: 0.48680272698402405\n",
      "TWO_TOWER_1: Epoch 147 finished. Loss: 0.48573699593544006\n",
      "TWO_TOWER_1: Epoch 148 finished. Loss: 0.4842931032180786\n",
      "TWO_TOWER_1: Epoch 149 finished. Loss: 0.48385119438171387\n",
      "TWO_TOWER_1: Epoch 150 finished. Loss: 0.48332682251930237\n",
      "TWO_TOWER_1: Epoch 151 finished. Loss: 0.48238834738731384\n",
      "TWO_TOWER_1: Epoch 152 finished. Loss: 0.48055756092071533\n",
      "TWO_TOWER_1: Epoch 153 finished. Loss: 0.48055291175842285\n",
      "TWO_TOWER_1: Epoch 154 finished. Loss: 0.47984060645103455\n",
      "TWO_TOWER_1: Epoch 155 finished. Loss: 0.4782170355319977\n",
      "TWO_TOWER_1: Epoch 156 finished. Loss: 0.47731277346611023\n",
      "TWO_TOWER_1: Epoch 157 finished. Loss: 0.4773707985877991\n",
      "TWO_TOWER_1: Epoch 158 finished. Loss: 0.47619637846946716\n",
      "TWO_TOWER_1: Epoch 159 finished. Loss: 0.47574496269226074\n",
      "TWO_TOWER_1: Epoch 160 finished. Loss: 0.47417324781417847\n",
      "TWO_TOWER_1: Epoch 161 finished. Loss: 0.4737662672996521\n",
      "TWO_TOWER_1: Epoch 162 finished. Loss: 0.47324639558792114\n",
      "TWO_TOWER_1: Epoch 163 finished. Loss: 0.47249656915664673\n",
      "TWO_TOWER_1: Epoch 164 finished. Loss: 0.4722960293292999\n",
      "TWO_TOWER_1: Epoch 165 finished. Loss: 0.4709302484989166\n",
      "TWO_TOWER_1: Epoch 166 finished. Loss: 0.470594584941864\n",
      "TWO_TOWER_1: Epoch 167 finished. Loss: 0.47002530097961426\n",
      "TWO_TOWER_1: Epoch 168 finished. Loss: 0.469440758228302\n",
      "TWO_TOWER_1: Epoch 169 finished. Loss: 0.46811750531196594\n",
      "TWO_TOWER_1: Epoch 170 finished. Loss: 0.46833619475364685\n",
      "TWO_TOWER_1: Epoch 171 finished. Loss: 0.4679841995239258\n",
      "TWO_TOWER_1: Epoch 172 finished. Loss: 0.46640223264694214\n",
      "TWO_TOWER_1: Epoch 173 finished. Loss: 0.46557456254959106\n",
      "TWO_TOWER_1: Epoch 174 finished. Loss: 0.46607914566993713\n",
      "TWO_TOWER_1: Epoch 175 finished. Loss: 0.4649968147277832\n",
      "TWO_TOWER_1: Epoch 176 finished. Loss: 0.46427369117736816\n",
      "TWO_TOWER_1: Epoch 177 finished. Loss: 0.4645758271217346\n",
      "TWO_TOWER_1: Epoch 178 finished. Loss: 0.46355384588241577\n",
      "TWO_TOWER_1: Epoch 179 finished. Loss: 0.46281662583351135\n",
      "TWO_TOWER_1: Epoch 180 finished. Loss: 0.4621037244796753\n",
      "TWO_TOWER_1: Epoch 181 finished. Loss: 0.4615819454193115\n",
      "TWO_TOWER_1: Epoch 182 finished. Loss: 0.46058350801467896\n",
      "TWO_TOWER_1: Epoch 183 finished. Loss: 0.46072983741760254\n",
      "TWO_TOWER_1: Epoch 184 finished. Loss: 0.46026018261909485\n",
      "TWO_TOWER_1: Epoch 185 finished. Loss: 0.459727942943573\n",
      "TWO_TOWER_1: Epoch 186 finished. Loss: 0.45917707681655884\n",
      "TWO_TOWER_1: Epoch 187 finished. Loss: 0.45863646268844604\n",
      "TWO_TOWER_1: Epoch 188 finished. Loss: 0.45839622616767883\n",
      "TWO_TOWER_1: Epoch 189 finished. Loss: 0.45805948972702026\n",
      "TWO_TOWER_1: Epoch 190 finished. Loss: 0.45731133222579956\n",
      "TWO_TOWER_1: Epoch 191 finished. Loss: 0.45650923252105713\n",
      "TWO_TOWER_1: Epoch 192 finished. Loss: 0.4570087194442749\n",
      "TWO_TOWER_1: Epoch 193 finished. Loss: 0.4560040533542633\n",
      "TWO_TOWER_1: Epoch 194 finished. Loss: 0.45572859048843384\n",
      "TWO_TOWER_1: Epoch 195 finished. Loss: 0.45431411266326904\n",
      "TWO_TOWER_1: Epoch 196 finished. Loss: 0.4543924927711487\n",
      "TWO_TOWER_1: Epoch 197 finished. Loss: 0.4541471302509308\n",
      "TWO_TOWER_1: Epoch 198 finished. Loss: 0.45294949412345886\n",
      "TWO_TOWER_1: Epoch 199 finished. Loss: 0.45220810174942017\n",
      "TWO_TOWER_1: Epoch 200 finished. Loss: 0.4529293477535248\n",
      "TWO_TOWER_1: Epoch 201 finished. Loss: 0.45236971974372864\n",
      "TWO_TOWER_1: Epoch 202 finished. Loss: 0.451809287071228\n",
      "TWO_TOWER_1: Epoch 203 finished. Loss: 0.45121803879737854\n",
      "TWO_TOWER_1: Epoch 204 finished. Loss: 0.45146286487579346\n",
      "TWO_TOWER_1: Epoch 205 finished. Loss: 0.45039960741996765\n",
      "TWO_TOWER_1: Epoch 206 finished. Loss: 0.4505310356616974\n",
      "TWO_TOWER_1: Epoch 207 finished. Loss: 0.4497116208076477\n",
      "TWO_TOWER_1: Epoch 208 finished. Loss: 0.44961434602737427\n",
      "TWO_TOWER_1: Epoch 209 finished. Loss: 0.4488413631916046\n",
      "TWO_TOWER_1: Epoch 210 finished. Loss: 0.44802916049957275\n",
      "TWO_TOWER_1: Epoch 211 finished. Loss: 0.44820326566696167\n",
      "TWO_TOWER_1: Epoch 212 finished. Loss: 0.44712430238723755\n",
      "TWO_TOWER_1: Epoch 213 finished. Loss: 0.44727423787117004\n",
      "TWO_TOWER_1: Epoch 214 finished. Loss: 0.44695785641670227\n",
      "TWO_TOWER_1: Epoch 215 finished. Loss: 0.447445809841156\n",
      "TWO_TOWER_1: Epoch 216 finished. Loss: 0.44658198952674866\n",
      "TWO_TOWER_1: Epoch 217 finished. Loss: 0.446793794631958\n",
      "TWO_TOWER_1: Epoch 218 finished. Loss: 0.44621095061302185\n",
      "TWO_TOWER_1: Epoch 219 finished. Loss: 0.44611966609954834\n",
      "TWO_TOWER_1: Epoch 220 finished. Loss: 0.4452909827232361\n",
      "TWO_TOWER_1: Epoch 221 finished. Loss: 0.4448928236961365\n",
      "TWO_TOWER_1: Epoch 222 finished. Loss: 0.44512394070625305\n",
      "TWO_TOWER_1: Epoch 223 finished. Loss: 0.44504278898239136\n",
      "TWO_TOWER_1: Epoch 224 finished. Loss: 0.4446917772293091\n",
      "TWO_TOWER_1: Epoch 225 finished. Loss: 0.4440053105354309\n",
      "TWO_TOWER_1: Epoch 226 finished. Loss: 0.44390785694122314\n",
      "TWO_TOWER_1: Epoch 227 finished. Loss: 0.4436638653278351\n",
      "TWO_TOWER_1: Epoch 228 finished. Loss: 0.44274085760116577\n",
      "TWO_TOWER_1: Epoch 229 finished. Loss: 0.44271135330200195\n",
      "TWO_TOWER_1: Epoch 230 finished. Loss: 0.44265511631965637\n",
      "TWO_TOWER_1: Epoch 231 finished. Loss: 0.44186145067214966\n",
      "TWO_TOWER_1: Epoch 232 finished. Loss: 0.4418780207633972\n",
      "TWO_TOWER_1: Epoch 233 finished. Loss: 0.4422798454761505\n",
      "TWO_TOWER_1: Epoch 234 finished. Loss: 0.441358357667923\n",
      "TWO_TOWER_1: Epoch 235 finished. Loss: 0.4411972165107727\n",
      "TWO_TOWER_1: Epoch 236 finished. Loss: 0.4409927725791931\n",
      "TWO_TOWER_1: Epoch 237 finished. Loss: 0.4420090615749359\n",
      "TWO_TOWER_1: Epoch 238 finished. Loss: 0.4410446882247925\n",
      "TWO_TOWER_1: Epoch 239 finished. Loss: 0.44082388281822205\n",
      "TWO_TOWER_1: Epoch 240 finished. Loss: 0.441025048494339\n",
      "TWO_TOWER_1: Epoch 241 finished. Loss: 0.4404791295528412\n",
      "TWO_TOWER_1: Epoch 242 finished. Loss: 0.4399856626987457\n",
      "TWO_TOWER_1: Epoch 243 finished. Loss: 0.4401412010192871\n",
      "TWO_TOWER_1: Epoch 244 finished. Loss: 0.43910762667655945\n",
      "TWO_TOWER_1: Epoch 245 finished. Loss: 0.43974632024765015\n",
      "TWO_TOWER_1: Epoch 246 finished. Loss: 0.4396549165248871\n",
      "TWO_TOWER_1: Epoch 247 finished. Loss: 0.4392845034599304\n",
      "TWO_TOWER_1: Epoch 248 finished. Loss: 0.43907442688941956\n",
      "TWO_TOWER_1: Epoch 249 finished. Loss: 0.43891963362693787\n",
      "TWO_TOWER_1: Epoch 250 finished. Loss: 0.4391786754131317\n",
      "TWO_TOWER_1: Epoch 251 finished. Loss: 0.4383145570755005\n",
      "TWO_TOWER_1: Epoch 252 finished. Loss: 0.4383982717990875\n",
      "TWO_TOWER_1: Epoch 253 finished. Loss: 0.4383567273616791\n",
      "TWO_TOWER_1: Epoch 254 finished. Loss: 0.43758663535118103\n",
      "TWO_TOWER_1: Epoch 255 finished. Loss: 0.4377683103084564\n",
      "TWO_TOWER_1: Epoch 256 finished. Loss: 0.4373144805431366\n",
      "TWO_TOWER_1: Epoch 257 finished. Loss: 0.437083899974823\n",
      "TWO_TOWER_1: Epoch 258 finished. Loss: 0.4375048279762268\n",
      "TWO_TOWER_1: Epoch 259 finished. Loss: 0.43719494342803955\n",
      "TWO_TOWER_1: Epoch 260 finished. Loss: 0.43690025806427\n",
      "TWO_TOWER_1: Epoch 261 finished. Loss: 0.43654561042785645\n",
      "TWO_TOWER_1: Epoch 262 finished. Loss: 0.4364558160305023\n",
      "TWO_TOWER_1: Epoch 263 finished. Loss: 0.4362857937812805\n",
      "TWO_TOWER_1: Epoch 264 finished. Loss: 0.43660682439804077\n",
      "TWO_TOWER_1: Epoch 265 finished. Loss: 0.43583768606185913\n",
      "TWO_TOWER_1: Epoch 266 finished. Loss: 0.4358759820461273\n",
      "TWO_TOWER_1: Epoch 267 finished. Loss: 0.435935914516449\n",
      "TWO_TOWER_1: Epoch 268 finished. Loss: 0.4355194568634033\n",
      "TWO_TOWER_1: Epoch 269 finished. Loss: 0.435642272233963\n",
      "TWO_TOWER_1: Epoch 270 finished. Loss: 0.434367299079895\n",
      "TWO_TOWER_1: Epoch 271 finished. Loss: 0.43499189615249634\n",
      "TWO_TOWER_1: Epoch 272 finished. Loss: 0.4346523880958557\n",
      "TWO_TOWER_1: Epoch 273 finished. Loss: 0.43413975834846497\n",
      "TWO_TOWER_1: Epoch 274 finished. Loss: 0.43393826484680176\n",
      "TWO_TOWER_1: Epoch 275 finished. Loss: 0.43437424302101135\n",
      "TWO_TOWER_1: Epoch 276 finished. Loss: 0.4343765079975128\n",
      "TWO_TOWER_1: Epoch 277 finished. Loss: 0.4341939687728882\n",
      "TWO_TOWER_1: Epoch 278 finished. Loss: 0.43376755714416504\n",
      "TWO_TOWER_1: Epoch 279 finished. Loss: 0.4328515827655792\n",
      "TWO_TOWER_1: Epoch 280 finished. Loss: 0.4329897463321686\n",
      "TWO_TOWER_1: Epoch 281 finished. Loss: 0.4335152208805084\n",
      "TWO_TOWER_1: Epoch 282 finished. Loss: 0.43247514963150024\n",
      "TWO_TOWER_1: Epoch 283 finished. Loss: 0.4323231279850006\n",
      "TWO_TOWER_1: Epoch 284 finished. Loss: 0.4329507350921631\n",
      "TWO_TOWER_1: Epoch 285 finished. Loss: 0.43211495876312256\n",
      "TWO_TOWER_1: Epoch 286 finished. Loss: 0.4319993853569031\n",
      "TWO_TOWER_1: Epoch 287 finished. Loss: 0.43174391984939575\n",
      "TWO_TOWER_1: Epoch 288 finished. Loss: 0.4318790137767792\n",
      "TWO_TOWER_1: Epoch 289 finished. Loss: 0.4314829707145691\n",
      "TWO_TOWER_1: Epoch 290 finished. Loss: 0.43226158618927\n",
      "TWO_TOWER_1: Epoch 291 finished. Loss: 0.4321109354496002\n",
      "TWO_TOWER_1: Epoch 292 finished. Loss: 0.4308113157749176\n",
      "TWO_TOWER_1: Epoch 293 finished. Loss: 0.43113821744918823\n",
      "TWO_TOWER_1: Epoch 294 finished. Loss: 0.43133923411369324\n",
      "TWO_TOWER_1: Epoch 295 finished. Loss: 0.431400865316391\n",
      "TWO_TOWER_1: Epoch 296 finished. Loss: 0.43068382143974304\n",
      "TWO_TOWER_1: Epoch 297 finished. Loss: 0.43008604645729065\n",
      "TWO_TOWER_1: Epoch 298 finished. Loss: 0.43045541644096375\n",
      "TWO_TOWER_1: Epoch 299 finished. Loss: 0.43100255727767944\n",
      "EvaluatorHoldout: Processed 35000 (50.1%) in 5.00 min. Users per second: 117\n",
      "EvaluatorHoldout: Processed 69803 (100.0%) in 9.99 min. Users per second: 116\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRECISION</th>\n",
       "      <th>PRECISION_RECALL_MIN_DEN</th>\n",
       "      <th>RECALL</th>\n",
       "      <th>MAP</th>\n",
       "      <th>MAP_MIN_DEN</th>\n",
       "      <th>MRR</th>\n",
       "      <th>NDCG</th>\n",
       "      <th>F1</th>\n",
       "      <th>HIT_RATE</th>\n",
       "      <th>ARHR_ALL_HITS</th>\n",
       "      <th>...</th>\n",
       "      <th>COVERAGE_USER</th>\n",
       "      <th>COVERAGE_USER_HIT</th>\n",
       "      <th>USERS_IN_GT</th>\n",
       "      <th>DIVERSITY_GINI</th>\n",
       "      <th>SHANNON_ENTROPY</th>\n",
       "      <th>RATIO_DIVERSITY_HERFINDAHL</th>\n",
       "      <th>RATIO_DIVERSITY_GINI</th>\n",
       "      <th>RATIO_SHANNON_ENTROPY</th>\n",
       "      <th>RATIO_AVERAGE_POPULARITY</th>\n",
       "      <th>RATIO_NOVELTY</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cutoff</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.129374</td>\n",
       "      <td>0.142682</td>\n",
       "      <td>0.068596</td>\n",
       "      <td>0.065975</td>\n",
       "      <td>0.071362</td>\n",
       "      <td>0.298375</td>\n",
       "      <td>0.114438</td>\n",
       "      <td>0.089655</td>\n",
       "      <td>0.598628</td>\n",
       "      <td>0.434521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998927</td>\n",
       "      <td>0.597985</td>\n",
       "      <td>0.998927</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>4.478696</td>\n",
       "      <td>0.947075</td>\n",
       "      <td>0.008718</td>\n",
       "      <td>0.395494</td>\n",
       "      <td>2.764258</td>\n",
       "      <td>0.082079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PRECISION PRECISION_RECALL_MIN_DEN    RECALL       MAP MAP_MIN_DEN  \\\n",
       "cutoff                                                                      \n",
       "10      0.129374                 0.142682  0.068596  0.065975    0.071362   \n",
       "\n",
       "             MRR      NDCG        F1  HIT_RATE ARHR_ALL_HITS  ...  \\\n",
       "cutoff                                                        ...   \n",
       "10      0.298375  0.114438  0.089655  0.598628      0.434521  ...   \n",
       "\n",
       "       COVERAGE_USER COVERAGE_USER_HIT USERS_IN_GT DIVERSITY_GINI  \\\n",
       "cutoff                                                              \n",
       "10          0.998927          0.597985    0.998927       0.001697   \n",
       "\n",
       "       SHANNON_ENTROPY RATIO_DIVERSITY_HERFINDAHL RATIO_DIVERSITY_GINI  \\\n",
       "cutoff                                                                   \n",
       "10            4.478696                   0.947075             0.008718   \n",
       "\n",
       "       RATIO_SHANNON_ENTROPY RATIO_AVERAGE_POPULARITY RATIO_NOVELTY  \n",
       "cutoff                                                               \n",
       "10                  0.395494                 2.764258      0.082079  \n",
       "\n",
       "[1 rows x 27 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and test type 1\n",
    "twotower_1 = TwoTowerRecommender_type1(URM_train, URM_train.shape[0], URM_train.shape[1], layers=[10,5,2,2], reg_layers=[0,0,0,0])\n",
    "\n",
    "twotower_1.fit(epochs=100, batch_size=1024, learning_rate=0.01)\n",
    "\n",
    "results_df, _ = evaluator_test.evaluateRecommender(twotower_1)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TWO_TOWER_2: URM Detected 76 ( 0.7%) items with no interactions.\n",
      "TWO_TOWER_2: Epoch 0 finished. Loss: 0.7077656388282776\n",
      "TWO_TOWER_2: Epoch 1 finished. Loss: 0.7031283974647522\n",
      "TWO_TOWER_2: Epoch 2 finished. Loss: 0.6991095542907715\n",
      "TWO_TOWER_2: Epoch 3 finished. Loss: 0.6958059668540955\n",
      "TWO_TOWER_2: Epoch 4 finished. Loss: 0.6929289698600769\n",
      "TWO_TOWER_2: Epoch 5 finished. Loss: 0.690422773361206\n",
      "TWO_TOWER_2: Epoch 6 finished. Loss: 0.6882591247558594\n",
      "TWO_TOWER_2: Epoch 7 finished. Loss: 0.6863564848899841\n",
      "TWO_TOWER_2: Epoch 8 finished. Loss: 0.6846165657043457\n",
      "TWO_TOWER_2: Epoch 9 finished. Loss: 0.6830786466598511\n",
      "TWO_TOWER_2: Epoch 10 finished. Loss: 0.6816461086273193\n",
      "TWO_TOWER_2: Epoch 11 finished. Loss: 0.6802965998649597\n",
      "TWO_TOWER_2: Epoch 12 finished. Loss: 0.678999125957489\n",
      "TWO_TOWER_2: Epoch 13 finished. Loss: 0.6777475476264954\n",
      "TWO_TOWER_2: Epoch 14 finished. Loss: 0.676535964012146\n",
      "TWO_TOWER_2: Epoch 15 finished. Loss: 0.6753504276275635\n",
      "TWO_TOWER_2: Epoch 16 finished. Loss: 0.6741893291473389\n",
      "TWO_TOWER_2: Epoch 17 finished. Loss: 0.6730362772941589\n",
      "TWO_TOWER_2: Epoch 18 finished. Loss: 0.6719003915786743\n",
      "TWO_TOWER_2: Epoch 19 finished. Loss: 0.6707764863967896\n",
      "TWO_TOWER_2: Epoch 20 finished. Loss: 0.6696628928184509\n",
      "TWO_TOWER_2: Epoch 21 finished. Loss: 0.6685587167739868\n",
      "TWO_TOWER_2: Epoch 22 finished. Loss: 0.6674644947052002\n",
      "TWO_TOWER_2: Epoch 23 finished. Loss: 0.6663795113563538\n",
      "TWO_TOWER_2: Epoch 24 finished. Loss: 0.6653034687042236\n",
      "TWO_TOWER_2: Epoch 25 finished. Loss: 0.664236843585968\n",
      "TWO_TOWER_2: Epoch 26 finished. Loss: 0.663178563117981\n",
      "TWO_TOWER_2: Epoch 27 finished. Loss: 0.6621283292770386\n",
      "TWO_TOWER_2: Epoch 28 finished. Loss: 0.661086916923523\n",
      "TWO_TOWER_2: Epoch 29 finished. Loss: 0.6600534915924072\n",
      "TWO_TOWER_2: Epoch 30 finished. Loss: 0.6590285897254944\n",
      "TWO_TOWER_2: Epoch 31 finished. Loss: 0.6580120921134949\n",
      "TWO_TOWER_2: Epoch 32 finished. Loss: 0.6570034623146057\n",
      "TWO_TOWER_2: Epoch 33 finished. Loss: 0.6560031175613403\n",
      "TWO_TOWER_2: Epoch 34 finished. Loss: 0.6550108194351196\n",
      "TWO_TOWER_2: Epoch 35 finished. Loss: 0.6540263891220093\n",
      "TWO_TOWER_2: Epoch 36 finished. Loss: 0.6530500650405884\n",
      "TWO_TOWER_2: Epoch 37 finished. Loss: 0.6520822644233704\n",
      "TWO_TOWER_2: Epoch 38 finished. Loss: 0.6511217355728149\n",
      "TWO_TOWER_2: Epoch 39 finished. Loss: 0.6501697301864624\n",
      "TWO_TOWER_2: Epoch 40 finished. Loss: 0.6492258310317993\n",
      "TWO_TOWER_2: Epoch 41 finished. Loss: 0.6482892632484436\n",
      "TWO_TOWER_2: Epoch 42 finished. Loss: 0.6473610401153564\n",
      "TWO_TOWER_2: Epoch 43 finished. Loss: 0.6464405655860901\n",
      "TWO_TOWER_2: Epoch 44 finished. Loss: 0.6455279588699341\n",
      "TWO_TOWER_2: Epoch 45 finished. Loss: 0.6446232795715332\n",
      "TWO_TOWER_2: Epoch 46 finished. Loss: 0.6437253952026367\n",
      "TWO_TOWER_2: Epoch 47 finished. Loss: 0.6428366899490356\n",
      "TWO_TOWER_2: Epoch 48 finished. Loss: 0.6419545412063599\n",
      "TWO_TOWER_2: Epoch 49 finished. Loss: 0.6410807371139526\n",
      "TWO_TOWER_2: Epoch 50 finished. Loss: 0.6402142643928528\n",
      "TWO_TOWER_2: Epoch 51 finished. Loss: 0.6393554210662842\n",
      "TWO_TOWER_2: Epoch 52 finished. Loss: 0.6385043859481812\n",
      "TWO_TOWER_2: Epoch 53 finished. Loss: 0.6376608610153198\n",
      "TWO_TOWER_2: Epoch 54 finished. Loss: 0.636824369430542\n",
      "TWO_TOWER_2: Epoch 55 finished. Loss: 0.6359965801239014\n",
      "TWO_TOWER_2: Epoch 56 finished. Loss: 0.6351757049560547\n",
      "TWO_TOWER_2: Epoch 57 finished. Loss: 0.6343618035316467\n",
      "TWO_TOWER_2: Epoch 58 finished. Loss: 0.6335550546646118\n",
      "TWO_TOWER_2: Epoch 59 finished. Loss: 0.6327565908432007\n",
      "TWO_TOWER_2: Epoch 60 finished. Loss: 0.6319642066955566\n",
      "TWO_TOWER_2: Epoch 61 finished. Loss: 0.6311805248260498\n",
      "TWO_TOWER_2: Epoch 62 finished. Loss: 0.6304022669792175\n",
      "TWO_TOWER_2: Epoch 63 finished. Loss: 0.6296328902244568\n",
      "TWO_TOWER_2: Epoch 64 finished. Loss: 0.6288692951202393\n",
      "TWO_TOWER_2: Epoch 65 finished. Loss: 0.6281144618988037\n",
      "TWO_TOWER_2: Epoch 66 finished. Loss: 0.6273646950721741\n",
      "TWO_TOWER_2: Epoch 67 finished. Loss: 0.6266226172447205\n",
      "TWO_TOWER_2: Epoch 68 finished. Loss: 0.6258876323699951\n",
      "TWO_TOWER_2: Epoch 69 finished. Loss: 0.6251602172851562\n",
      "TWO_TOWER_2: Epoch 70 finished. Loss: 0.6244391202926636\n",
      "TWO_TOWER_2: Epoch 71 finished. Loss: 0.6237243413925171\n",
      "TWO_TOWER_2: Epoch 72 finished. Loss: 0.6230179071426392\n",
      "TWO_TOWER_2: Epoch 73 finished. Loss: 0.6223167181015015\n",
      "TWO_TOWER_2: Epoch 74 finished. Loss: 0.6216228008270264\n",
      "TWO_TOWER_2: Epoch 75 finished. Loss: 0.6209352016448975\n",
      "TWO_TOWER_2: Epoch 76 finished. Loss: 0.62025386095047\n",
      "TWO_TOWER_2: Epoch 77 finished. Loss: 0.6195791363716125\n",
      "TWO_TOWER_2: Epoch 78 finished. Loss: 0.6189120411872864\n",
      "TWO_TOWER_2: Epoch 79 finished. Loss: 0.6182515621185303\n",
      "TWO_TOWER_2: Epoch 80 finished. Loss: 0.6175965666770935\n",
      "TWO_TOWER_2: Epoch 81 finished. Loss: 0.6169480681419373\n",
      "TWO_TOWER_2: Epoch 82 finished. Loss: 0.6163056492805481\n",
      "TWO_TOWER_2: Epoch 83 finished. Loss: 0.6156691312789917\n",
      "TWO_TOWER_2: Epoch 84 finished. Loss: 0.61504065990448\n",
      "TWO_TOWER_2: Epoch 85 finished. Loss: 0.6144173741340637\n",
      "TWO_TOWER_2: Epoch 86 finished. Loss: 0.6138007640838623\n",
      "TWO_TOWER_2: Epoch 87 finished. Loss: 0.613189160823822\n",
      "TWO_TOWER_2: Epoch 88 finished. Loss: 0.6125844120979309\n",
      "TWO_TOWER_2: Epoch 89 finished. Loss: 0.6119835376739502\n",
      "TWO_TOWER_2: Epoch 90 finished. Loss: 0.6113917827606201\n",
      "TWO_TOWER_2: Epoch 91 finished. Loss: 0.6108053922653198\n",
      "TWO_TOWER_2: Epoch 92 finished. Loss: 0.6102250218391418\n",
      "TWO_TOWER_2: Epoch 93 finished. Loss: 0.6096484661102295\n",
      "TWO_TOWER_2: Epoch 94 finished. Loss: 0.6090790629386902\n",
      "TWO_TOWER_2: Epoch 95 finished. Loss: 0.6085163354873657\n",
      "TWO_TOWER_2: Epoch 96 finished. Loss: 0.6079578399658203\n",
      "TWO_TOWER_2: Epoch 97 finished. Loss: 0.6074053049087524\n",
      "TWO_TOWER_2: Epoch 98 finished. Loss: 0.6068590879440308\n",
      "TWO_TOWER_2: Epoch 99 finished. Loss: 0.6063196659088135\n",
      "EvaluatorHoldout: Processed 30000 (43.0%) in 5.10 min. Users per second: 98\n",
      "EvaluatorHoldout: Processed 60000 (86.0%) in 10.21 min. Users per second: 98\n",
      "EvaluatorHoldout: Processed 69803 (100.0%) in 11.97 min. Users per second: 97\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRECISION</th>\n",
       "      <th>PRECISION_RECALL_MIN_DEN</th>\n",
       "      <th>RECALL</th>\n",
       "      <th>MAP</th>\n",
       "      <th>MAP_MIN_DEN</th>\n",
       "      <th>MRR</th>\n",
       "      <th>NDCG</th>\n",
       "      <th>F1</th>\n",
       "      <th>HIT_RATE</th>\n",
       "      <th>ARHR_ALL_HITS</th>\n",
       "      <th>...</th>\n",
       "      <th>COVERAGE_USER</th>\n",
       "      <th>COVERAGE_USER_HIT</th>\n",
       "      <th>USERS_IN_GT</th>\n",
       "      <th>DIVERSITY_GINI</th>\n",
       "      <th>SHANNON_ENTROPY</th>\n",
       "      <th>RATIO_DIVERSITY_HERFINDAHL</th>\n",
       "      <th>RATIO_DIVERSITY_GINI</th>\n",
       "      <th>RATIO_SHANNON_ENTROPY</th>\n",
       "      <th>RATIO_AVERAGE_POPULARITY</th>\n",
       "      <th>RATIO_NOVELTY</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cutoff</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.002679</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998927</td>\n",
       "      <td>0.002676</td>\n",
       "      <td>0.998927</td>\n",
       "      <td>0.003194</td>\n",
       "      <td>4.063549</td>\n",
       "      <td>0.919136</td>\n",
       "      <td>0.016413</td>\n",
       "      <td>0.358834</td>\n",
       "      <td>0.008847</td>\n",
       "      <td>0.167405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PRECISION PRECISION_RECALL_MIN_DEN    RECALL       MAP MAP_MIN_DEN  \\\n",
       "cutoff                                                                      \n",
       "10      0.000284                 0.000284  0.000045  0.000096    0.000097   \n",
       "\n",
       "             MRR      NDCG        F1  HIT_RATE ARHR_ALL_HITS  ...  \\\n",
       "cutoff                                                        ...   \n",
       "10      0.000906  0.000161  0.000077  0.002679      0.000933  ...   \n",
       "\n",
       "       COVERAGE_USER COVERAGE_USER_HIT USERS_IN_GT DIVERSITY_GINI  \\\n",
       "cutoff                                                              \n",
       "10          0.998927          0.002676    0.998927       0.003194   \n",
       "\n",
       "       SHANNON_ENTROPY RATIO_DIVERSITY_HERFINDAHL RATIO_DIVERSITY_GINI  \\\n",
       "cutoff                                                                   \n",
       "10            4.063549                   0.919136             0.016413   \n",
       "\n",
       "       RATIO_SHANNON_ENTROPY RATIO_AVERAGE_POPULARITY RATIO_NOVELTY  \n",
       "cutoff                                                               \n",
       "10                  0.358834                 0.008847      0.167405  \n",
       "\n",
       "[1 rows x 27 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and test type 2\n",
    "twotower_2 = TwoTowerRecommender_type2(URM_train, URM_train.shape[0], URM_train.shape[1], layers=[10,5,2,2], reg_layers=[0,0,0,0])\n",
    "\n",
    "twotower_2.fit(epochs=100, batch_size=1024, learning_rate=0.01)\n",
    "\n",
    "results_df, _ = evaluator_test.evaluateRecommender(twotower_2)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGCN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computer(self):\n",
    "        \"\"\"\n",
    "        propagate methods for lightGCN\n",
    "        \"\"\"\n",
    "        users_emb = self.embedding_user.weight\n",
    "        items_emb = self.embedding_item.weight\n",
    "        all_emb = torch.cat([users_emb, items_emb])\n",
    "        embs = [all_emb]\n",
    "        if self.dropout_rate > 0.0:\n",
    "            if self.training:\n",
    "                g_dropped = self.__dropout(1 - self.dropout_rate)\n",
    "            else:\n",
    "                g_dropped = self.Graph\n",
    "        else:\n",
    "            g_dropped = self.Graph\n",
    "\n",
    "        for layer in range(self.n_layers):\n",
    "            all_emb = torch.sparse.mm(g_dropped, all_emb) # <- G * all_emb\n",
    "            embs.append(all_emb) # <- Collect results\n",
    "        embs = torch.stack(embs, dim=1)\n",
    "        light_out = torch.mean(embs, dim=1) # <- Output = mean of collection\n",
    "        users, items = torch.split(light_out, [self.n_users, self.n_items])\n",
    "        return users, items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, users, items):\n",
    "        # compute embedding\n",
    "        all_users, all_items = self.computer()\n",
    "        users_emb = all_users[users]\n",
    "        items_emb = all_items[items]\n",
    "        inner_pro = torch.mul(users_emb, items_emb)\n",
    "        gamma = torch.sum(inner_pro, dim=1)\n",
    "        return gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GF-CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "def fit(self, alpha=1.0, num_factors=50, random_seed = None):\n",
    "        self._print(\"Computing SVD decomposition of the normalized adjacency matrix...\")\n",
    "\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.D_I = np.sqrt(np.array(self.URM_train.sum(axis = 0))).squeeze()\n",
    "        self.D_I_inv = 1/(self.D_I + 1e-6)\n",
    "        self.D_U_inv = 1/np.sqrt(np.array(self.URM_train.sum(axis = 1))).squeeze() + 1e-6\n",
    "\n",
    "        self.D_I = sp.diags(self.D_I)\n",
    "        self.D_I_inv = sp.diags(self.D_I_inv)\n",
    "        self.D_U_inv = sp.diags(self.D_U_inv)\n",
    "\n",
    "        self.R_tilde = self.D_U_inv.dot(self.URM_train).dot(self.D_I_inv)\n",
    "\n",
    "        _, _, self.V = randomized_svd(self.R_tilde,\n",
    "                                     n_components = num_factors,\n",
    "                                     random_state = random_seed)\n",
    "\n",
    "        self.D_I = sp.csr_matrix(self.D_I)\n",
    "        self.D_I_inv = sp.csr_matrix(self.D_I_inv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RSFramework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
